"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[9465],{4462:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"module-2/sensor-integration","title":"Sensor Integration and Perception","description":"Introduction","source":"@site/docs/module-2/sensor-integration.md","sourceDirName":"module-2","slug":"/module-2/sensor-integration","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-2/sensor-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2/sensor-integration.md","tags":[],"version":"current","frontMatter":{"title":"Sensor Integration and Perception","week":6,"module":2},"sidebar":"tutorialSidebar","previous":{"title":"Digital Twins in Robotics","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-2/digital-twins"},"next":{"title":"Overview","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-2-digital-twin/"}}');var s=i(4848),l=i(8453);const o={title:"Sensor Integration and Perception",week:6,module:2},t="Sensor Integration and Perception",c={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Types of Sensors",id:"types-of-sensors",level:2},{value:"Vision Sensors",id:"vision-sensors",level:3},{value:"Proximity Sensors",id:"proximity-sensors",level:3},{value:"Inertial Sensors",id:"inertial-sensors",level:3},{value:"Force and Tactile Sensors",id:"force-and-tactile-sensors",level:3},{value:"Other Sensors",id:"other-sensors",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Why Sensor Fusion?",id:"why-sensor-fusion",level:3},{value:"Fusion Techniques",id:"fusion-techniques",level:3},{value:"Perception Pipeline",id:"perception-pipeline",level:2},{value:"Image Processing",id:"image-processing",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Object Detection",id:"object-detection",level:3},{value:"ROS 2 Sensor Integration",id:"ros-2-sensor-integration",level:2},{value:"Camera Integration",id:"camera-integration",level:3},{value:"LiDAR Integration",id:"lidar-integration",level:3},{value:"IMU Integration",id:"imu-integration",level:3},{value:"Calibration",id:"calibration",level:2},{value:"Camera Calibration",id:"camera-calibration",level:3},{value:"Sensor-to-Sensor Calibration",id:"sensor-to-sensor-calibration",level:3},{value:"Example: Chessboard Calibration",id:"example-chessboard-calibration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Resources",id:"resources",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"sensor-integration-and-perception",children:"Sensor Integration and Perception"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"Sensor integration is crucial for robots to perceive and understand their environment. This module covers various sensors used in robotics and how to process their data effectively."}),"\n",(0,s.jsx)(e.h2,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,s.jsx)(e.h3,{id:"vision-sensors",children:"Vision Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"RGB Cameras"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Color image acquisition"}),"\n",(0,s.jsx)(e.li,{children:"Object detection and recognition"}),"\n",(0,s.jsx)(e.li,{children:"Visual servoing"}),"\n",(0,s.jsx)(e.li,{children:"Common: USB cameras, industrial cameras"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Depth Cameras"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"3D perception"}),"\n",(0,s.jsx)(e.li,{children:"Obstacle avoidance"}),"\n",(0,s.jsx)(e.li,{children:"Object grasping"}),"\n",(0,s.jsx)(e.li,{children:"Technologies: Stereo, structured light, ToF (Time-of-Flight)"}),"\n",(0,s.jsx)(e.li,{children:"Examples: RealSense, Kinect, ZED"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"LiDAR (Light Detection and Ranging)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"360-degree scanning"}),"\n",(0,s.jsx)(e.li,{children:"Precise distance measurements"}),"\n",(0,s.jsx)(e.li,{children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,s.jsx)(e.li,{children:"Types: 2D (planar), 3D (spinning, solid-state)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"proximity-sensors",children:"Proximity Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Ultrasonic"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Short-range distance measurement (0.02-4m)"}),"\n",(0,s.jsx)(e.li,{children:"Low cost"}),"\n",(0,s.jsx)(e.li,{children:"Affected by surface materials"}),"\n",(0,s.jsx)(e.li,{children:"Common in mobile robots"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Infrared (IR)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Proximity detection"}),"\n",(0,s.jsx)(e.li,{children:"Line following"}),"\n",(0,s.jsx)(e.li,{children:"Object detection"}),"\n",(0,s.jsx)(e.li,{children:"Reflectivity-based"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Time-of-Flight"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Accurate distance measurement"}),"\n",(0,s.jsx)(e.li,{children:"Fast response time"}),"\n",(0,s.jsx)(e.li,{children:"Wide field of view"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"inertial-sensors",children:"Inertial Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"IMU (Inertial Measurement Unit)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Accelerometer: Linear acceleration"}),"\n",(0,s.jsx)(e.li,{children:"Gyroscope: Angular velocity"}),"\n",(0,s.jsx)(e.li,{children:"Magnetometer: Magnetic field direction"}),"\n",(0,s.jsx)(e.li,{children:"Sensor fusion for orientation estimation"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"GPS/GNSS"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Outdoor localization"}),"\n",(0,s.jsx)(e.li,{children:"Waypoint navigation"}),"\n",(0,s.jsx)(e.li,{children:"Accuracy: 1-10m (consumer), cm-level (RTK)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"force-and-tactile-sensors",children:"Force and Tactile Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Force/Torque Sensors"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Measure applied forces"}),"\n",(0,s.jsx)(e.li,{children:"Located at robot joints or end-effector"}),"\n",(0,s.jsx)(e.li,{children:"Compliance control"}),"\n",(0,s.jsx)(e.li,{children:"Safe human-robot interaction"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Tactile Sensors"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Surface contact detection"}),"\n",(0,s.jsx)(e.li,{children:"Texture recognition"}),"\n",(0,s.jsx)(e.li,{children:"Grasp stability"}),"\n",(0,s.jsx)(e.li,{children:"Types: Resistive, capacitive, optical"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"other-sensors",children:"Other Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Encoders"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Joint position measurement"}),"\n",(0,s.jsx)(e.li,{children:"Wheel odometry"}),"\n",(0,s.jsx)(e.li,{children:"Incremental vs. absolute"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Microphones"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Voice commands"}),"\n",(0,s.jsx)(e.li,{children:"Sound localization"}),"\n",(0,s.jsx)(e.li,{children:"Environmental monitoring"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Temperature Sensors"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Motor thermal protection"}),"\n",(0,s.jsx)(e.li,{children:"Environmental monitoring"}),"\n",(0,s.jsx)(e.li,{children:"Safety systems"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(e.h3,{id:"why-sensor-fusion",children:"Why Sensor Fusion?"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Redundancy"}),": Multiple sensors provide backup"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Complementary Information"}),": Combine strengths of different sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Improved Accuracy"}),": Reduce individual sensor errors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness"}),": Handle sensor failures gracefully"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"fusion-techniques",children:"Fusion Techniques"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Kalman Filter"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Optimal estimation for linear systems"}),"\n",(0,s.jsx)(e.li,{children:"Predict-update cycle"}),"\n",(0,s.jsx)(e.li,{children:"Commonly used for IMU + GPS fusion"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class KalmanFilter:\r\n    def __init__(self, A, H, Q, R):\r\n        self.A = A  # State transition matrix\r\n        self.H = H  # Observation matrix\r\n        self.Q = Q  # Process noise covariance\r\n        self.R = R  # Measurement noise covariance\r\n\r\n    def predict(self, x, P):\r\n        x = self.A @ x\r\n        P = self.A @ P @ self.A.T + self.Q\r\n        return x, P\r\n\r\n    def update(self, x, P, z):\r\n        y = z - self.H @ x  # Innovation\r\n        S = self.H @ P @ self.H.T + self.R\r\n        K = P @ self.H.T @ np.linalg.inv(S)  # Kalman gain\r\n        x = x + K @ y\r\n        P = (np.eye(len(x)) - K @ self.H) @ P\r\n        return x, P\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Extended Kalman Filter (EKF)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Handles non-linear systems"}),"\n",(0,s.jsx)(e.li,{children:"Linearization around current estimate"}),"\n",(0,s.jsx)(e.li,{children:"Used in robot localization"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Particle Filter"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Non-parametric Bayesian filter"}),"\n",(0,s.jsx)(e.li,{children:"Represents belief as set of particles"}),"\n",(0,s.jsx)(e.li,{children:"Good for multi-modal distributions"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Complementary Filter"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Simple and computationally efficient"}),"\n",(0,s.jsx)(e.li,{children:"Combines high-frequency and low-frequency data"}),"\n",(0,s.jsx)(e.li,{children:"Common for IMU attitude estimation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,s.jsx)(e.h3,{id:"image-processing",children:"Image Processing"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Acquisition"}),": Capture image from camera"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Preprocessing"}),": Denoise, enhance, correct distortion"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feature Extraction"}),": Edges, corners, blobs"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Segmentation"}),": Separate objects from background"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Recognition"}),": Identify objects, classify"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Common Libraries"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"OpenCV: Computer vision algorithms"}),"\n",(0,s.jsx)(e.li,{children:"PIL/Pillow: Image manipulation"}),"\n",(0,s.jsx)(e.li,{children:"scikit-image: Image processing"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Acquisition"}),": LiDAR or depth camera"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Filtering"}),": Remove noise and outliers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Downsampling"}),": Reduce computational load"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Segmentation"}),": Cluster points into objects"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Registration"}),": Align multiple clouds"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Recognition"}),": Match to known models"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Common Libraries"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"PCL (Point Cloud Library)"}),"\n",(0,s.jsx)(e.li,{children:"Open3D"}),"\n",(0,s.jsx)(e.li,{children:"ROS perception packages"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"object-detection",children:"Object Detection"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Classical Methods"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Haar Cascades"}),"\n",(0,s.jsx)(e.li,{children:"HOG (Histogram of Oriented Gradients)"}),"\n",(0,s.jsx)(e.li,{children:"SIFT, SURF features"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Deep Learning Methods"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"YOLO (You Only Look Once): Real-time detection"}),"\n",(0,s.jsx)(e.li,{children:"SSD (Single Shot Detector)"}),"\n",(0,s.jsx)(e.li,{children:"Faster R-CNN: High accuracy"}),"\n",(0,s.jsx)(e.li,{children:"EfficientDet: Balanced speed/accuracy"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# YOLO example with ROS 2\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO('yolov8n.pt')\r\n\r\ndef detect_objects(image):\r\n    results = model(image)\r\n    detections = []\r\n\r\n    for result in results:\r\n        for box in result.boxes:\r\n            detection = {\r\n                'class': model.names[int(box.cls)],\r\n                'confidence': float(box.conf),\r\n                'bbox': box.xyxy[0].tolist()\r\n            }\r\n            detections.append(detection)\r\n\r\n    return detections\n"})}),"\n",(0,s.jsx)(e.h2,{id:"ros-2-sensor-integration",children:"ROS 2 Sensor Integration"}),"\n",(0,s.jsx)(e.h3,{id:"camera-integration",children:"Camera Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\n\r\nclass CameraNode(Node):\r\n    def __init__(self):\r\n        super().__init__('camera_node')\r\n        self.publisher = self.create_publisher(Image, '/camera/image_raw', 10)\r\n        self.timer = self.create_timer(0.033, self.publish_image)  # 30 FPS\r\n        self.bridge = CvBridge()\r\n        self.camera = cv2.VideoCapture(0)\r\n\r\n    def publish_image(self):\r\n        ret, frame = self.camera.read()\r\n        if ret:\r\n            msg = self.bridge.cv2_to_imgmsg(frame, encoding='bgr8')\r\n            msg.header.stamp = self.get_clock().now().to_msg()\r\n            msg.header.frame_id = 'camera_frame'\r\n            self.publisher.publish(msg)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"lidar-integration",children:"LiDAR Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from sensor_msgs.msg import LaserScan\r\n\r\nclass LidarNode(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_node')\r\n        self.publisher = self.create_publisher(LaserScan, '/scan', 10)\r\n        self.timer = self.create_timer(0.1, self.publish_scan)  # 10 Hz\r\n\r\n    def publish_scan(self):\r\n        scan = LaserScan()\r\n        scan.header.stamp = self.get_clock().now().to_msg()\r\n        scan.header.frame_id = 'laser_frame'\r\n        scan.angle_min = -math.pi\r\n        scan.angle_max = math.pi\r\n        scan.angle_increment = math.pi / 180\r\n        scan.range_min = 0.1\r\n        scan.range_max = 30.0\r\n        scan.ranges = self.get_lidar_ranges()  # From sensor\r\n        self.publisher.publish(scan)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"imu-integration",children:"IMU Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\r\n\r\nclass ImuNode(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_node')\r\n        self.publisher = self.create_publisher(Imu, '/imu/data', 10)\r\n        self.timer = self.create_timer(0.01, self.publish_imu)  # 100 Hz\r\n\r\n    def publish_imu(self):\r\n        imu_msg = Imu()\r\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\r\n        imu_msg.header.frame_id = 'imu_link'\r\n\r\n        # Orientation (from sensor fusion)\r\n        imu_msg.orientation.x, imu_msg.orientation.y = ...\r\n\r\n        # Angular velocity (from gyroscope)\r\n        imu_msg.angular_velocity.x, imu_msg.angular_velocity.y = ...\r\n\r\n        # Linear acceleration (from accelerometer)\r\n        imu_msg.linear_acceleration.x, imu_msg.linear_acceleration.y = ...\r\n\r\n        self.publisher.publish(imu_msg)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"calibration",children:"Calibration"}),"\n",(0,s.jsx)(e.h3,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Correct lens distortion"}),"\n",(0,s.jsx)(e.li,{children:"Estimate intrinsic parameters (focal length, principal point)"}),"\n",(0,s.jsx)(e.li,{children:"Estimate extrinsic parameters (position, orientation)"}),"\n",(0,s.jsx)(e.li,{children:"Tools: OpenCV calibration, ROS camera_calibration"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"sensor-to-sensor-calibration",children:"Sensor-to-Sensor Calibration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Determine relative positions between sensors"}),"\n",(0,s.jsx)(e.li,{children:"Hand-eye calibration for camera-robot"}),"\n",(0,s.jsx)(e.li,{children:"LiDAR-camera calibration for fusion"}),"\n",(0,s.jsx)(e.li,{children:"IMU-camera synchronization"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"example-chessboard-calibration",children:"Example: Chessboard Calibration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import cv2\r\nimport numpy as np\r\n\r\n# Prepare object points\r\nobjp = np.zeros((6*9, 3), np.float32)\r\nobjp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\r\n\r\nobjpoints = []  # 3D points\r\nimgpoints = []  # 2D points\r\n\r\n# Find chessboard corners\r\nfor image in calibration_images:\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n    ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)\r\n\r\n    if ret:\r\n        objpoints.append(objp)\r\n        imgpoints.append(corners)\r\n\r\n# Calibrate camera\r\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\r\n    objpoints, imgpoints, gray.shape[::-1], None, None)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor Selection"}),": Choose sensors based on task requirements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Redundancy"}),": Use multiple sensors for critical measurements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Calibration"}),": Regularly calibrate sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Data Validation"}),": Check sensor data for errors and outliers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Synchronization"}),": Timestamp and synchronize multi-sensor data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Power Management"}),": Monitor sensor power consumption"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environmental Factors"}),": Account for lighting, weather, interference"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"OpenCV Documentation: docs.opencv.org"}),"\n",(0,s.jsx)(e.li,{children:"PCL Tutorials: pointclouds.org/documentation/tutorials"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 Perception: docs.ros.org/en/rolling/Tutorials"}),"\n",(0,s.jsx)(e.li,{children:"Sensor Datasheets: Manufacturer websites"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,l.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var r=i(6540);const s={},l=r.createContext(s);function o(n){const e=r.useContext(l);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(l.Provider,{value:e},n.children)}}}]);