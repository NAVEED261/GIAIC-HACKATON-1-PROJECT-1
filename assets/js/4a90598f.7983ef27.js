"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[3601],{5301:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action Models","description":"Integrate VLA models for natural language-driven humanoid robot control and decision-making","source":"@site/docs/module-4-vla/index.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Module 4: Vision-Language-Action Models","description":"Integrate VLA models for natural language-driven humanoid robot control and decision-making","sidebar_label":"Overview","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Robotics","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-4/humanoid-robotics"},"next":{"title":"Overview","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/capstone/"}}');var t=i(4848),s=i(8453);const a={title:"Module 4: Vision-Language-Action Models",description:"Integrate VLA models for natural language-driven humanoid robot control and decision-making",sidebar_label:"Overview",sidebar_position:4},l="Module 4: Vision-Language-Action (VLA) Models",r={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Content",id:"module-content",level:2},{value:"Week 11: VLA Introduction",id:"week-11-vla-introduction",level:3},{value:"Week 12: Humanoid Integration",id:"week-12-humanoid-integration",level:3},{value:"Week 13: Capstone Integration",id:"week-13-capstone-integration",level:3},{value:"Capstone Integration",id:"capstone-integration",level:2},{value:"Capstone Project",id:"capstone-project",level:2}];function c(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"module-4-vision-language-action-vla-models",children:"Module 4: Vision-Language-Action (VLA) Models"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"In the final module (Weeks 11-13), you'll integrate Vision-Language-Action models to enable natural language understanding and high-level decision-making for autonomous humanoid robots."}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Duration:"})," 3 weeks (30 hours estimated)\r\n",(0,t.jsx)(e.strong,{children:"Weeks:"})," 11-13\r\n",(0,t.jsx)(e.strong,{children:"Prerequisites:"})," Modules 1-3, basic understanding of transformer models"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understand VLA Architecture"})," - Explain how VLAs combine vision, language, and action for robotics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Integrate Natural Language"})," - Enable robots to understand and execute natural language commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Implement Humanoid Control"})," - Deploy VLAs for humanoid robot task planning and execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Complete Capstone Project"})," - Integrate all modules into an autonomous humanoid system"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"module-content",children:"Module Content"}),"\n",(0,t.jsx)(e.h3,{id:"week-11-vla-introduction",children:"Week 11: VLA Introduction"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"VLA model architecture"}),"\n",(0,t.jsx)(e.li,{children:"Pre-trained models for robotics"}),"\n",(0,t.jsx)(e.li,{children:"Language-to-action mapping"}),"\n",(0,t.jsx)(e.li,{children:"Fine-tuning for specific tasks"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"week-12-humanoid-integration",children:"Week 12: Humanoid Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Humanoid robot control with VLAs"}),"\n",(0,t.jsx)(e.li,{children:"Task decomposition from natural language"}),"\n",(0,t.jsx)(e.li,{children:"Integration with ROS 2 and Isaac Sim"}),"\n",(0,t.jsx)(e.li,{children:"Multi-modal perception and action"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"week-13-capstone-integration",children:"Week 13: Capstone Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Complete autonomous humanoid system"}),"\n",(0,t.jsx)(e.li,{children:"Voice interface implementation"}),"\n",(0,t.jsx)(e.li,{children:"Task planning and execution"}),"\n",(0,t.jsx)(e.li,{children:"Final capstone project presentation"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"capstone-integration",children:"Capstone Integration"}),"\n",(0,t.jsxs)(e.p,{children:["VLAs provide the ",(0,t.jsx)(e.strong,{children:"high-level intelligence"})," for your humanoid:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'Understand natural language commands ("Bring me the red cup")'}),"\n",(0,t.jsx)(e.li,{children:"Decompose tasks into robot actions"}),"\n",(0,t.jsx)(e.li,{children:"Plan multi-step manipulation sequences"}),"\n",(0,t.jsx)(e.li,{children:"Adapt to dynamic environments"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"capstone-project",children:"Capstone Project"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Autonomous Humanoid System (Week 13)"})}),"\n",(0,t.jsx)(e.p,{children:"Build a complete system with 5 components:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Voice Interface"})," - Speech recognition and natural language understanding"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Planner"})," - VLA-based task decomposition"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation"})," - Autonomous navigation to target locations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception"})," - Object detection and pose estimation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manipulation"})," - Grasp planning and execution"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.p,{children:["Begin your journey with ",(0,t.jsx)(e.strong,{children:"Week 11: VLA Fundamentals"})," \u2192"]})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);