"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7435],{6370:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>t,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"module-4/vla-models","title":"Vision-Language-Action (VLA) Models","description":"What are VLA Models?","source":"@site/docs/module-4/vla-models.md","sourceDirName":"module-4","slug":"/module-4/vla-models","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-4/vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/vla-models.md","tags":[],"version":"current","frontMatter":{"title":"Vision-Language-Action (VLA) Models","week":11,"module":4},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-3-isaac/"},"next":{"title":"Humanoid Robotics","permalink":"/GIAIC-HACKATON-1-PROJECT-1/docs/module-4/humanoid-robotics"}}');var r=i(4848),s=i(8453);const o={title:"Vision-Language-Action (VLA) Models",week:11,module:4},a="Vision-Language-Action (VLA) Models",t={},d=[{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"How VLA Models Work",id:"how-vla-models-work",level:2},{value:"Training Pipeline",id:"training-pipeline",level:3},{value:"Inference Process",id:"inference-process",level:3},{value:"Popular VLA Models",id:"popular-vla-models",level:2},{value:"RT-1 (Robotics Transformer 1)",id:"rt-1-robotics-transformer-1",level:3},{value:"RT-2 (Robotics Transformer 2)",id:"rt-2-robotics-transformer-2",level:3},{value:"PaLM-E",id:"palm-e",level:3},{value:"RoboFlamingo",id:"roboflamingo",level:3},{value:"Key Capabilities",id:"key-capabilities",level:2},{value:"Instruction Following",id:"instruction-following",level:3},{value:"Generalization",id:"generalization",level:3},{value:"Common Sense Reasoning",id:"common-sense-reasoning",level:3},{value:"Long-Horizon Planning",id:"long-horizon-planning",level:3},{value:"Implementation Example",id:"implementation-example",level:2},{value:"Using a Pre-trained VLA Model",id:"using-a-pre-trained-vla-model",level:3},{value:"Training a Custom VLA Model",id:"training-a-custom-vla-model",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:2},{value:"Data Requirements",id:"data-requirements",level:3},{value:"Computational Cost",id:"computational-cost",level:3},{value:"Safety and Reliability",id:"safety-and-reliability",level:3},{value:"Sim-to-Real Gap",id:"sim-to-real-gap",level:3},{value:"Future Directions",id:"future-directions",level:2},{value:"Foundation Models for Robotics",id:"foundation-models-for-robotics",level:3},{value:"Multimodal Fusion",id:"multimodal-fusion",level:3},{value:"Interactive Learning",id:"interactive-learning",level:3},{value:"Embodied AI",id:"embodied-ai",level:3},{value:"Datasets for VLA Training",id:"datasets-for-vla-training",level:2},{value:"Open X-Embodiment",id:"open-x-embodiment",level:3},{value:"RoboNet",id:"robonet",level:3},{value:"BC-Z",id:"bc-z",level:3},{value:"Applications",id:"applications",level:2},{value:"Manufacturing",id:"manufacturing",level:3},{value:"Healthcare",id:"healthcare",level:3},{value:"Home Robotics",id:"home-robotics",level:3},{value:"Agriculture",id:"agriculture",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Resources",id:"resources",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"vision-language-action-vla-models",children:"Vision-Language-Action (VLA) Models"})}),"\n",(0,r.jsx)(e.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,r.jsx)(e.p,{children:"Vision-Language-Action (VLA) models are a new class of AI models that integrate visual perception, natural language understanding, and robotic control into a single end-to-end system. They enable robots to understand multimodal instructions and execute complex tasks."}),"\n",(0,r.jsx)(e.h2,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Vision Encoder"}),": Processes visual input (images/video)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,r.jsx)(e.li,{children:"Vision Transformers (ViT)"}),"\n",(0,r.jsx)(e.li,{children:"Pre-trained models (CLIP, DINOv2)"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Language Encoder"}),": Understands natural language commands"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Large Language Models (LLMs)"}),"\n",(0,r.jsx)(e.li,{children:"BERT, GPT, T5 architectures"}),"\n",(0,r.jsx)(e.li,{children:"Instruction parsing and grounding"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Action Decoder"}),": Generates robot control commands"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Policy networks"}),"\n",(0,r.jsx)(e.li,{children:"Trajectory generation"}),"\n",(0,r.jsx)(e.li,{children:"Low-level motor control"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Fusion Module"}),": Combines vision and language"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Cross-attention mechanisms"}),"\n",(0,r.jsx)(e.li,{children:"Multimodal transformers"}),"\n",(0,r.jsx)(e.li,{children:"Contrastive learning"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"how-vla-models-work",children:"How VLA Models Work"}),"\n",(0,r.jsx)(e.h3,{id:"training-pipeline",children:"Training Pipeline"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Pretraining on Large Datasets"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Internet-scale vision-language data"}),"\n",(0,r.jsx)(e.li,{children:"Robot demonstration datasets"}),"\n",(0,r.jsx)(e.li,{children:"Transfer learning from foundation models"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Task-Specific Fine-tuning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Domain-specific robot tasks"}),"\n",(0,r.jsx)(e.li,{children:"Environment-specific scenarios"}),"\n",(0,r.jsx)(e.li,{children:"Safety and constraint learning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Reinforcement Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Policy optimization in simulation"}),"\n",(0,r.jsx)(e.li,{children:"Real-world fine-tuning"}),"\n",(0,r.jsx)(e.li,{children:"Online adaptation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"inference-process",children:"Inference Process"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Input"}),": Natural language instruction + Visual observation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Perception"}),": Extract visual features and language semantics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reasoning"}),": Understand task requirements and constraints"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Planning"}),": Determine sequence of actions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Execution"}),": Generate low-level control commands"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"popular-vla-models",children:"Popular VLA Models"}),"\n",(0,r.jsx)(e.h3,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer 1)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Google's VLA model for robotic manipulation"}),"\n",(0,r.jsx)(e.li,{children:"Trained on 130,000 demonstrations"}),"\n",(0,r.jsx)(e.li,{children:"700+ tasks across multiple robots"}),"\n",(0,r.jsx)(e.li,{children:"Tokenizes images and actions"}),"\n",(0,r.jsx)(e.li,{children:"Transformer-based architecture"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"rt-2-robotics-transformer-2",children:"RT-2 (Robotics Transformer 2)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Builds on vision-language models (PaLI-X)"}),"\n",(0,r.jsx)(e.li,{children:"Web-scale knowledge transfer to robotics"}),"\n",(0,r.jsx)(e.li,{children:"Generalizes to novel objects and instructions"}),"\n",(0,r.jsx)(e.li,{children:"Chain-of-thought reasoning for robots"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"palm-e",children:"PaLM-E"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Multimodal embodied language model"}),"\n",(0,r.jsx)(e.li,{children:"562 billion parameters"}),"\n",(0,r.jsx)(e.li,{children:"Integrates sensor data into LLM"}),"\n",(0,r.jsx)(e.li,{children:"Multi-task learning across robotics domains"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"roboflamingo",children:"RoboFlamingo"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Open-source VLA model"}),"\n",(0,r.jsx)(e.li,{children:"Vision-language-action learning"}),"\n",(0,r.jsx)(e.li,{children:"Few-shot imitation learning"}),"\n",(0,r.jsx)(e.li,{children:"Handles diverse robot embodiments"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"key-capabilities",children:"Key Capabilities"}),"\n",(0,r.jsx)(e.h3,{id:"instruction-following",children:"Instruction Following"}),"\n",(0,r.jsx)(e.p,{children:"Execute complex, multi-step commands:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'"Pick up the blue cube and place it in the red bowl"'}),"\n",(0,r.jsx)(e.li,{children:'"Open the drawer and retrieve the marker"'}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"generalization",children:"Generalization"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Novel objects not seen during training"}),"\n",(0,r.jsx)(e.li,{children:"New environments and layouts"}),"\n",(0,r.jsx)(e.li,{children:"Variations in instructions and phrasing"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"common-sense-reasoning",children:"Common Sense Reasoning"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understanding object properties (heavy, fragile)"}),"\n",(0,r.jsx)(e.li,{children:"Spatial relationships (on, under, next to)"}),"\n",(0,r.jsx)(e.li,{children:"Task affordances and constraints"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"long-horizon-planning",children:"Long-Horizon Planning"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Breaking down complex tasks into subtasks"}),"\n",(0,r.jsx)(e.li,{children:"Handling failures and retrying"}),"\n",(0,r.jsx)(e.li,{children:"Adaptive execution based on feedback"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"implementation-example",children:"Implementation Example"}),"\n",(0,r.jsx)(e.h3,{id:"using-a-pre-trained-vla-model",children:"Using a Pre-trained VLA Model"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import torch\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\n# Load VLA model\r\nmodel = AutoModel.from_pretrained("google/rt-1-x")\r\ntokenizer = AutoTokenizer.from_pretrained("google/rt-1-x")\r\n\r\n# Process instruction\r\ninstruction = "Pick up the red block"\r\ntext_inputs = tokenizer(instruction, return_tensors="pt")\r\n\r\n# Process image\r\nfrom PIL import Image\r\nimage = Image.open("robot_view.jpg")\r\nimage_tensor = preprocess_image(image)\r\n\r\n# Generate action\r\nwith torch.no_grad():\r\n    action = model(\r\n        image=image_tensor,\r\n        text=text_inputs\r\n    )\r\n\r\n# Execute on robot\r\nrobot.execute_action(action)\n'})}),"\n",(0,r.jsx)(e.h3,{id:"training-a-custom-vla-model",children:"Training a Custom VLA Model"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from vla_training import VLATrainer\r\n\r\n# Define model architecture\r\nmodel = VLAModel(\r\n    vision_encoder="dinov2",\r\n    language_encoder="t5-base",\r\n    action_dim=7  # 7-DOF robot arm\r\n)\r\n\r\n# Prepare dataset\r\ndataset = RobotDemonstrationDataset(\r\n    data_path="demos/",\r\n    augmentation=True\r\n)\r\n\r\n# Train\r\ntrainer = VLATrainer(model, dataset)\r\ntrainer.train(\r\n    epochs=100,\r\n    batch_size=32,\r\n    learning_rate=1e-4\r\n)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,r.jsx)(e.h3,{id:"data-requirements",children:"Data Requirements"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Need large-scale robot demonstration data"}),"\n",(0,r.jsx)(e.li,{children:"Expensive and time-consuming to collect"}),"\n",(0,r.jsx)(e.li,{children:"Distribution shift between training and deployment"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"computational-cost",children:"Computational Cost"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Large model sizes (billions of parameters)"}),"\n",(0,r.jsx)(e.li,{children:"High inference latency"}),"\n",(0,r.jsx)(e.li,{children:"GPU/TPU requirements"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"safety-and-reliability",children:"Safety and Reliability"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Unpredictable behavior on out-of-distribution inputs"}),"\n",(0,r.jsx)(e.li,{children:"Difficulty in providing safety guarantees"}),"\n",(0,r.jsx)(e.li,{children:"Interpretability and debugging challenges"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"sim-to-real-gap",children:"Sim-to-Real Gap"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Models trained in simulation may not transfer well"}),"\n",(0,r.jsx)(e.li,{children:"Domain adaptation required"}),"\n",(0,r.jsx)(e.li,{children:"Real-world variability and noise"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,r.jsx)(e.h3,{id:"foundation-models-for-robotics",children:"Foundation Models for Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Pre-trained on massive robot datasets"}),"\n",(0,r.jsx)(e.li,{children:"Transfer learning across tasks and embodiments"}),"\n",(0,r.jsx)(e.li,{children:"Universal robot policies"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"multimodal-fusion",children:"Multimodal Fusion"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Incorporating tactile, audio, and proprioceptive data"}),"\n",(0,r.jsx)(e.li,{children:"Better world models and representations"}),"\n",(0,r.jsx)(e.li,{children:"Improved situational awareness"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"interactive-learning",children:"Interactive Learning"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Learning from human feedback"}),"\n",(0,r.jsx)(e.li,{children:"Online adaptation and fine-tuning"}),"\n",(0,r.jsx)(e.li,{children:"Active learning strategies"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"embodied-ai",children:"Embodied AI"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Integrating VLA models with physical robots"}),"\n",(0,r.jsx)(e.li,{children:"Real-world deployment at scale"}),"\n",(0,r.jsx)(e.li,{children:"Home and industrial applications"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"datasets-for-vla-training",children:"Datasets for VLA Training"}),"\n",(0,r.jsx)(e.h3,{id:"open-x-embodiment",children:"Open X-Embodiment"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"1 million+ robot demonstrations"}),"\n",(0,r.jsx)(e.li,{children:"22 institutions, 21 robot types"}),"\n",(0,r.jsx)(e.li,{children:"Diverse tasks and environments"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"robonet",children:"RoboNet"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Large-scale robot learning dataset"}),"\n",(0,r.jsx)(e.li,{children:"Multiple viewpoints and modalities"}),"\n",(0,r.jsx)(e.li,{children:"Self-supervised learning"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"bc-z",children:"BC-Z"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Berkeley's robotic manipulation dataset"}),"\n",(0,r.jsx)(e.li,{children:"Language-annotated demonstrations"}),"\n",(0,r.jsx)(e.li,{children:"Diverse object interactions"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"applications",children:"Applications"}),"\n",(0,r.jsx)(e.h3,{id:"manufacturing",children:"Manufacturing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Flexible assembly lines"}),"\n",(0,r.jsx)(e.li,{children:"Quality inspection with language feedback"}),"\n",(0,r.jsx)(e.li,{children:"Collaborative robots (cobots)"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"healthcare",children:"Healthcare"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Assistive robotics for elderly care"}),"\n",(0,r.jsx)(e.li,{children:"Surgical assistance"}),"\n",(0,r.jsx)(e.li,{children:"Medication delivery"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"home-robotics",children:"Home Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Household chores (cleaning, cooking)"}),"\n",(0,r.jsx)(e.li,{children:"Object fetching and organization"}),"\n",(0,r.jsx)(e.li,{children:"Personalized assistance"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"agriculture",children:"Agriculture"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Crop monitoring and harvesting"}),"\n",(0,r.jsx)(e.li,{children:"Precision agriculture"}),"\n",(0,r.jsx)(e.li,{children:"Automated weeding and planting"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Start with Pre-trained Models"}),": Leverage existing VLA models"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Domain Adaptation"}),": Fine-tune on your specific tasks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Data Augmentation"}),": Increase robustness with synthetic data"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety Constraints"}),": Implement hard-coded safety rules"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Human Oversight"}),": Keep human-in-the-loop for critical tasks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Iterative Improvement"}),": Continuously collect data and retrain"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Open X-Embodiment: robotics-transformer-x.github.io"}),"\n",(0,r.jsx)(e.li,{children:"RT-2 Paper: arxiv.org/abs/2307.15818"}),"\n",(0,r.jsx)(e.li,{children:"RoboFlamingo: roboflamingo.github.io"}),"\n",(0,r.jsx)(e.li,{children:"Google Robotics: ai.google/research/robotics"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>a});var l=i(6540);const r={},s=l.createContext(r);function o(n){const e=l.useContext(s);return l.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),l.createElement(s.Provider,{value:e},n.children)}}}]);